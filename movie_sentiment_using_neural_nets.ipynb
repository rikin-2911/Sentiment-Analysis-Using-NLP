{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Movie Sentiments using the basic Neural Networks Architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1. Gather and Exploring the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the pandas library for viewing the data into the pandas dataframe.\n",
    "import pandas as pd\n",
    "raw_df = pd.read_csv('IMDB Dataset.csv')\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'positive' to 1 and 'negative' to 0\n",
    "raw_df['sentiment'] = raw_df['sentiment'].replace({'positive': 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's gather some more information of our data.\n",
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing and analysing the probability distribution of the Sentiments across the dataset.\n",
    "raw_df.sentiment.value_counts(normalize=True).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It looks like we have balanced data with 50-50 Probability of both the Sentiments i.e., Positive and Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now let's create the Vocabulary from our data using text pre-processing techniques like TF-IDF.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization and Stemming of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the nltk library for implementing this task.\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize # For Tokening the words from the sentences.\n",
    "from nltk.stem.snowball import SnowballStemmer #For stemming the tokens.\n",
    "stemmer = SnowballStemmer(language='english') #instance of stemming class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's go with some of the stopwords, as this are one of the important factors in the sentiment analysis.\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "print(\", \".join(english_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the first 115 words which will certainly have most of the negative words..\n",
    "selected_stopwords = english_stopwords[:100]\n",
    "print(\", \".join(selected_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we are using the TF-IDF method for vectorisation of text.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's define the helper function for implementing the both tokenization and stemming..\n",
    "def tokenize(text):\n",
    "    return [stemmer.stem(word) for word in word_tokenize(text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the TF-IDF vectorizer for making the vocabulary for our model..\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize,\n",
    "                             stop_words=selected_stopwords,\n",
    "                             ngram_range=(1,2),\n",
    "                             max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(raw_df.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the Sentiments into the vectors using the tf-idf vectorizer\n",
    "inputs = vectorizer.transform(raw_df.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of the inputs\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let's split our data into Training, Validation and Test Sets.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_val_inputs, test_inputs, train_val_targets, test_targets = train_test_split(inputs, raw_df.sentiment, test_size=0.15, random_state=29)\n",
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(train_val_inputs, train_val_targets, test_size=0.15, random_state=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the dimensions of the training set, validation set and test set..\n",
    "f\"{train_inputs.shape}, {train_targets.shape} | {val_inputs.shape}, {val_targets.shape} | {test_inputs.shape}, {test_targets.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's convert the training, val and test into PyTorch Tensors as our Deep Learning model needs Tensors to Work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the torch module which is a PyTorch framework..\n",
    "# Converting the inputs into the tensors..\n",
    "import torch\n",
    "train_input_tensors = torch.tensor(train_inputs.toarray()).float()\n",
    "val_input_tensors = torch.tensor(val_inputs.toarray()).float()\n",
    "test_input_tensors = torch.tensor(test_inputs.toarray()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the shape of the input tensors..\n",
    "f\"{train_input_tensors.shape} | {val_input_tensors.shape} | {test_input_tensors.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the targets into the tensors..\n",
    "train_target_tensors = torch.tensor(train_targets.values).float()\n",
    "val_target_tensors = torch.tensor(val_targets.values).float()\n",
    "test_target_tensors = torch.tensor(test_targets.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the shape of the target tensors..\n",
    "f\"{train_target_tensors.shape} | {val_target_tensors.shape} | {test_target_tensors.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the PyTorch Dataset and PyTorch Dataloader for Batching the data.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1. Tensor Datasets --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the train, val and test tensor datasets..\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_ds = TensorDataset(train_input_tensors, train_target_tensors) \n",
    "val_ds = TensorDataset(val_input_tensors, val_target_tensors)\n",
    "test_ds = TensorDataset(test_input_tensors, test_target_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tensor dataloader for batching --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the batch size..\n",
    "BATCH_SIZE = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the train, val and test dataloaders..\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the length of train, val and test dataloaders..\n",
    "f\"{len(train_dl)} | {len(val_dl)} | {len(test_dl)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Working of Batches..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use the 'for-in' conditions while working with batches.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dl:\n",
    "    batch_inputs, batch_targets  = batch\n",
    "    print(\"batch_input_shape\", batch_inputs.shape)\n",
    "    print(\"batch_target_shape\", batch_targets.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now It's time to make our Deep Neural Networks.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the nn module for making the neural networks..\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # The loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the custom class for making the Deep Neural Networks..\n",
    "class IMDBNet(nn.Module):\n",
    "\n",
    "    # First defining the init method which requires only self as argument.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(5000, 2500) # Because we have 5000 TF-IDF vectors.\n",
    "        self.layer2 = nn.Linear(2500, 1250)\n",
    "        self.layer3 = nn.Linear(1250, 625)\n",
    "        self.layer4 = nn.Linear(625, 312)\n",
    "        self.layer5 = nn.Linear(312, 156)\n",
    "        self.layer6 = nn.Linear(156, 1)\n",
    "\n",
    "    # Defining the function for the forward pass..\n",
    "    def forward(self, inputs):\n",
    "        out = self.layer1(inputs)\n",
    "        out = F.relu(out)\n",
    "        out = self.layer2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.layer5(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.layer6(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of a model.\n",
    "model = IMDBNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dl:\n",
    "    bi, bt = batch\n",
    "    print('input_shape', bi.shape)\n",
    "    print('target_shape', bt.shape)\n",
    "\n",
    "    bo = model(bi)\n",
    "    print('bo_shape', bo.shape)\n",
    "\n",
    "    # Convert outputs to the probabilities\n",
    "    probs = torch.sigmoid(bo[:,0])\n",
    "    print(\"Probs: \", probs[:10])\n",
    "\n",
    "    # Convert probabilities to prediction\n",
    "    preds = (probs > 0.48).int()\n",
    "    print(\"Predictions: \", preds[:10])\n",
    "    print(\"Targets: \", bt[:10])\n",
    "\n",
    "    #Check metrics for evaluation..\n",
    "    print('Accuracy: ', accuracy_score(bt, preds))\n",
    "    print(\"F1-Score: \", f1_score(bt, preds, average='weighted'))\n",
    "\n",
    "    # Implementing the loss function for checking the loss..\n",
    "    print(\"Loss: \", F.binary_cross_entropy(preds.float(), bt))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual targets..\n",
    "bt[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs..\n",
    "bo[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let's make the Evulation Function for our model.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function..\n",
    "def evaluate(model, dl):\n",
    "\n",
    "    # Making the lists for appending the results of  accuracy, f1-score and loss.\n",
    "    losses, accs, f1s = [], [], []\n",
    "\n",
    "    # looping over the batches.\n",
    "    for batch in dl:\n",
    "        inputs, targets = batch \n",
    "\n",
    "        # Pass inputs to the model.\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Convert to the probabilites.\n",
    "        probs = torch.sigmoid(outputs[:,0])\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.binary_cross_entropy(probs, targets)\n",
    "\n",
    "        # Compute predictions.\n",
    "        preds = (probs > 0.48).int()\n",
    "\n",
    "        # Compute accuracy and F1-score.\n",
    "        acc = accuracy_score(targets, preds)\n",
    "        f1 = f1_score(targets, preds, average='weighted')\n",
    "\n",
    "        # Appending the loss, accuracy and f1-score 's.\n",
    "        losses.append(loss)\n",
    "        accs.append(acc)\n",
    "        f1s.append(f1) \n",
    "\n",
    "    return (torch.mean(torch.tensor(losses)).item(),\n",
    "           torch.mean(torch.tensor(accs)).item(),\n",
    "           torch.mean(torch.tensor(f1s)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train and fitting the model batch by batch.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the function..\n",
    "def fit(epochs, lr, model, train_dl, val_dl):\n",
    "\n",
    "    #Keeping the history.\n",
    "    history = []\n",
    "\n",
    "    # Optimization method\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr, weight_decay=1e-5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_dl:\n",
    "\n",
    "            # Get inputs and targets.\n",
    "            inputs, targets = batch\n",
    "\n",
    "            # Get model outputs.\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get probabilities.\n",
    "            probs = torch.sigmoid(outputs[:,0])\n",
    "\n",
    "            # loss function\n",
    "            loss = F.binary_cross_entropy(probs, targets)\n",
    "\n",
    "            # Doing mathematical calculations like gradients, etc.\n",
    "            loss.backward() # Back propagation.\n",
    "            optimizer.step() # Optimization.\n",
    "            optimizer.zero_grad() # Converting to zero gradients after one run.\n",
    "\n",
    "    # Evaluation of the model.\n",
    "    loss, acc, f1 = evaluate(model, val_dl)\n",
    "    print('Epoch : {}; Loss: {}; Accuracy: {}; F1-Score: {};'.format(epoch+1, loss, acc, f1))\n",
    "    history.append([loss, acc, f1]) \n",
    "    return history       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(5, 0.001, model, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's run the model again.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IMDBNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.append(evaluate(model, val_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history += fit(5, 0.001, model, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [item [0] for item in history]\n",
    "accs = [item[1] for item in history]\n",
    "f1s = [item[2] for item in history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Accuracy\")\n",
    "plt.plot(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"F1-Score\")\n",
    "plt.plot(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making prediction on an example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_df = raw_df.sample(10)\n",
    "ex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_df(df):\n",
    "    inputs = vectorizer.transform(df.review)\n",
    "    input_tensors = torch.tensor(inputs.toarray()).float()\n",
    "    outputs = model(input_tensors)\n",
    "    probs = torch.sigmoid(outputs[:,0])\n",
    "    preds = (probs > 0.48).int()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_df.sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df(ex_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_df.review"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
